{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEWpayxURuUn"
      },
      "source": [
        "# Crawl Data Twitter > 2000 Tweets\n",
        "The crawling process was done using Tweet-Harvest. Written by Helmi Satria on  March 30th 2024.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6S00x_f6-GeD"
      },
      "outputs": [],
      "source": [
        "#@title Twitter Auth Token\n",
        "\n",
        "twitter_auth_token = 'fe0b81858cc8f19e932101bb09b02c5f46078ea2' # change this auth token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4UIL1x21P9rQ",
        "outputId": "9ad83df8-1244-464b-9eff-54d78e787e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://deb.nodesource.com/node_20.x nodistro InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 2s (171 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20240203~22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.18).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "curl: (23) Failed writing body\n",
            "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://deb.nodesource.com/node_20.x nodistro InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "nodejs is already the newest version (20.17.0-1nodesource1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "v20.17.0\n"
          ]
        }
      ],
      "source": [
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYDR51dJlVlX",
        "outputId": "cc912cbf-2682-4e70-b872-a6daa7bb4de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K\u001b[1m\u001b[32mTweet Harvest [v2.6.1]\u001b[39m\u001b[22m\n",
            "\u001b[1m\u001b[32m\u001b[39m\u001b[22m\n",
            "\u001b[34mResearch by \u001b[39m\u001b[1m\u001b[34mHelmi Satria\u001b[39m\u001b[22m\u001b[34m\u001b[39m\n",
            "\u001b[34mUse it for Educational Purposes only!\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[33mThis script uses Chromium Browser to crawl data from Twitter with \u001b[1myour Twitter auth token\u001b[22m.\u001b[39m\n",
            "\u001b[33mPlease enter your Twitter auth token when prompted.\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m Keep your access token secret! Don't share it with anyone else.\n",
            "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m This script only runs on your local device.\n",
            "\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mOpening twitter search page...\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[90m (3)\u001b[39m\u001b[33m\u001b[39m\n",
            "\u001b[33mFilling in keywords: makanan manis since:2024-06-01 until:2024-07-01 lang:id\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[90m (4)\u001b[39m\u001b[31m\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@e70c31830956363100e3c2b8d2118519\"}\u001b[39m\n",
            "\u001b[31m\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\u001b[39m\n",
            "\u001b[31m\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@c797dbceddc68a1c60afa9fc6633b60c\"}\u001b[39m\n",
            "\u001b[31m\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\u001b[39m\n",
            "\u001b[31m\u001b[90m[v2.6.1]\u001b[39m Error parsing response json: {\"_type\":\"Response\",\"_guid\":\"response@b12aca45584e711d8db6a54968730d14\"}\u001b[39m\n",
            "\u001b[31m\u001b[90m[v2.6.1]\u001b[39m Most likely, you have already exceeded the Twitter rate limit. Read more on https://x.com/elonmusk/status/1675187969420828672.\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 18\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[90m (3)\u001b[39m\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 37\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 56\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 75\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 95\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[90m (3)\u001b[39m\u001b[90m (4)\u001b[39m\u001b[90m (5)\u001b[39m\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 115\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m--Taking a break, waiting for 10 seconds...\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 134\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 154\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[34m\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mYour tweets saved to: /content/tweets-data/makanan_manis_0624.csv\u001b[39m\n",
            "\u001b[33mTotal tweets saved: 174\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Crawl Data\n",
        "\n",
        "filename = 'makanan_manis_0624.csv'\n",
        "search_keyword = 'makanan manis since:2024-06-01 until:2024-07-01 lang:id'\n",
        "limit = 1000\n",
        "\n",
        "!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvAG3hPvQDqk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = f\"tweets-data/makanan_manis_0624.csv\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path, delimiter=\",\")\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRfDl54waHC4"
      },
      "outputs": [],
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}